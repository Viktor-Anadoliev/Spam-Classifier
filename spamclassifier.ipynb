{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Classifier\n",
    "\n",
    "This is a Naïve Bayes supervised learning based classifier to suggest whether a given email is spam or ham(not spam).\n",
    "\n",
    "## Training Data\n",
    "The training data is shown below and has 1000 rows including test data of 500 rows. Test data is functionally identical to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the spam training data set: (1000, 55)\n",
      "[[1 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 1 0 0]\n",
      " [0 0 0 ... 1 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 1]\n",
      " [1 1 1 ... 1 1 0]\n",
      " [1 0 0 ... 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from IPython.display import HTML,Javascript, display\n",
    "\n",
    "training_spam = np.loadtxt(open(\"data/training_spam.csv\"), delimiter=\",\").astype(int)\n",
    "print(\"Shape of the spam training data set:\", training_spam.shape)\n",
    "print(training_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training set consists of 1000 rows and 55 columns. Each row corresponds to one email message. The first column is the _response_ variable and describes whether a message is spam `1` or not `0`. The remaining 54 columns are _features_ that are used to build a classifier. These features correspond to 54 different keywords (such as \"money\", \"free\", and \"receive\") and special characters (such as \":\", \"!\", and \"$\"). A feature has the value `1` if the keyword appears in the message and `0` otherwise.\n",
    "\n",
    "As mentioned there is also a 500 row set of *test data*. It contains the same 55 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the spam testing data set: (500, 55)\n",
      "[[1 0 0 ... 1 1 1]\n",
      " [1 1 0 ... 1 1 1]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "testing_spam = np.loadtxt(open(\"data/testing_spam.csv\"), delimiter=\",\").astype(int)\n",
    "print(\"Shape of the spam testing data set:\", testing_spam.shape)\n",
    "print(testing_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier takes input data and returns class predictions. The input is a single $n \\times 54$ numpy array, the classifier returns a numpy array of length $n$ with classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamClassifier:\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        \n",
    "    def estimate_log_class_priors(self, data):\n",
    "        # extracting the class labels\n",
    "        class_labels = data[:, 0]\n",
    "\n",
    "        # counting the occurences of 0s and 1s in the left-most column\n",
    "        count_0s = np.sum(class_labels == 0)\n",
    "        count_1s = np.sum(class_labels == 1)\n",
    "\n",
    "        # finding the number of samples\n",
    "        n_samples = len(class_labels)\n",
    "\n",
    "        # calculating the logarithms of the empirical class priors (0s and 1s)\n",
    "        log_prob_c0 = np.log(count_0s / n_samples)\n",
    "        log_prob_c1 = np.log(count_1s / n_samples)\n",
    "\n",
    "        return np.array([log_prob_c0, log_prob_c1])\n",
    "\n",
    "    def estimate_log_class_conditional_likelihoods(self, data, alpha=1.0):\n",
    "        # find and seperate spam and ham messages in different arrays\n",
    "        ham_data = data[data[:, 0] == 0][:, 1:]\n",
    "        spam_data = data[data[:, 0] == 1][:, 1:]\n",
    "        #Include k? not self.k?\n",
    "        #k = data.shape[1] - 1\n",
    "\n",
    "        # counting the occurences of spam and ham messages\n",
    "        count_hams = len(ham_data)\n",
    "        count_spams = len(spam_data)\n",
    "\n",
    "        # calculate the number of times that each feature(word) appears in spam and ham messages\n",
    "        n_of_words_ham = ham_data.sum(axis=0)\n",
    "        n_of_words_spam = spam_data.sum(axis=0)\n",
    "\n",
    "        # calculating total number of words in spam and ham messages\n",
    "        total_n_of_words_ham = n_of_words_ham.sum()\n",
    "        total_n_of_words_spam = n_of_words_spam.sum()\n",
    "\n",
    "        theta_ham = []\n",
    "        for i in range(0, n_of_words_ham.shape[0]):\n",
    "            theta_ham.append(np.log((n_of_words_ham[i] + alpha)/(total_n_of_words_ham + self.k*alpha)))\n",
    "\n",
    "        theta_spam = []\n",
    "        for i in range(0, n_of_words_spam.shape[0]):\n",
    "            theta_spam.append(np.log((n_of_words_spam[i] + alpha)/(total_n_of_words_spam + self.k*alpha)))\n",
    "\n",
    "        theta = np.array([theta_ham, theta_spam])\n",
    "        return theta\n",
    "        \n",
    "    def train(self):\n",
    "        self.log_class_priors = self.estimate_log_class_priors(training_spam)\n",
    "        self.log_class_conditional_likelihoods = self.estimate_log_class_conditional_likelihoods(training_spam, alpha=1.0)\n",
    "        \n",
    "    def predict(self, new_data):\n",
    "        # finding the sum of the ham conditional likehoods multiplied by the words binary value in the new data (0 if not present)\n",
    "        ham_likelihoods_sum = np.dot(new_data[:,:], self.log_class_conditional_likelihoods[0])\n",
    "        # finding the numerator of probability that the message is ham\n",
    "        ham_results = self.log_class_priors[0] + ham_likelihoods_sum\n",
    "\n",
    "        # finding the sum of the spam conditional likehoods multiplied by the words binary value in the new data (0 if not present)\n",
    "        spam_likelihoods_sum = np.dot(new_data[:,:], self.log_class_conditional_likelihoods[1])\n",
    "        # finding the numerator of probability that the message is spam\n",
    "        spam_results = self.log_class_priors[1] + spam_likelihoods_sum\n",
    "\n",
    "        # calculating the maximum a posteriori estimate for each row(each message) and finding the results\n",
    "        class_predictions_ls = []\n",
    "        for i in range (0, new_data.shape[0]):\n",
    "            if ham_results[i] >= spam_results[i]:\n",
    "                class_predictions_ls.append(0)\n",
    "            else:\n",
    "                class_predictions_ls.append(1)\n",
    "        \n",
    "        # changing the type from list to np.array\n",
    "        class_predictions = np.array(class_predictions_ls)\n",
    "        return class_predictions\n",
    "\n",
    "    \n",
    "def create_classifier():\n",
    "    classifier = SpamClassifier(k=54)\n",
    "    classifier.train()\n",
    "    return classifier\n",
    "\n",
    "classifier = create_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Details\n",
    "The classifier is tested against some hidden data from the same source as the original. The accuracy is calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKIP_TESTS = True\n",
    "\n",
    "if not SKIP_TESTS:\n",
    "    testing_spam = np.loadtxt(open(\"data/testing_spam.csv\"), delimiter=\",\").astype(int)\n",
    "    test_data = testing_spam[:, 1:]\n",
    "    test_labels = testing_spam[:, 0]\n",
    "\n",
    "    predictions = classifier.predict(test_data)\n",
    "    accuracy = np.count_nonzero(predictions == test_labels)/test_labels.shape[0]\n",
    "    print(f\"Accuracy on test data is: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "57d38fa58f242d4d856fbaa18e9f8768",
     "grade": false,
     "grade_id": "cell-b6c47ab23c28b2b9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m[✓]\u001b[0m 'SKIP_TESTS' is set to true.\u001b[0m\n",
      "\u001b[1;32m[✓]\u001b[0m The notebook name is correct.\u001b[0m\n",
      "\u001b[1;32m[✓]\u001b[0m The create_classifier function has been defined.\u001b[0m\n",
      "\u001b[1;32m[✓]\u001b[0m The classifer variable has been correctly defined.\u001b[0m\n",
      "\u001b[1;32m[✓]\u001b[0m The my_accuracy_estimate function has been defined correctly.\u001b[0m\n",
      "\u001b[1;32m[✓]\u001b[0m Success running test set - Accuracy was 89.80%.\u001b[0m\n",
      "\u001b[1m\n",
      "\n",
      "\n",
      "╔═══════════════════════════════════════════════════════════════╗\n",
      "║                        Congratulations!                       ║\n",
      "║                                                               ║\n",
      "║            Your work meets all the required criteria          ║\n",
      "║                   and is ready for submission.                ║\n",
      "╚═══════════════════════════════════════════════════════════════╝\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "\n",
    "fail = False;\n",
    "\n",
    "success = '\\033[1;32m[✓]\\033[0m'\n",
    "issue = '\\033[1;33m[!]'\n",
    "error = '\\033[1;31m\\t✗'\n",
    "\n",
    "#######\n",
    "##\n",
    "## Skip Tests check.\n",
    "##\n",
    "## Test to ensure the SKIP_TESTS variable is set to True to prevent it slowing down the automarker.\n",
    "##\n",
    "#######\n",
    "\n",
    "if not SKIP_TESTS:\n",
    "    fail = True;\n",
    "    print(\"{} \\'SKIP_TESTS\\' is incorrectly set to False.\\033[0m\".format(issue))\n",
    "    print(\"{} You must set the SKIP_TESTS constant to True in the cell above.\\033[0m\".format(error))\n",
    "else:\n",
    "    print('{} \\'SKIP_TESTS\\' is set to true.\\033[0m'.format(success))\n",
    "\n",
    "#######\n",
    "##\n",
    "## File Name check.\n",
    "##\n",
    "## Test to ensure file has the correct name. This is important for the marking system to correctly process the submission.\n",
    "##\n",
    "#######\n",
    "    \n",
    "p3 = pathlib.Path('./spamclassifier.ipynb')\n",
    "if not p3.is_file():\n",
    "    fail = True\n",
    "    print(\"{} The notebook name is incorrect.\\033[0m\".format(issue))\n",
    "    print(\"{} This notebook file must be named spamclassifier.ipynb\\033[0m\".format(error))\n",
    "else:\n",
    "    print('{} The notebook name is correct.\\033[0m'.format(success))\n",
    "\n",
    "#######\n",
    "##\n",
    "## Create classifier function check.\n",
    "##\n",
    "## Test that checks the create_classifier function exists. The function should train the classifier and return it so that it can be evaluated by the marking system.\n",
    "##\n",
    "#######\n",
    "\n",
    "if \"create_classifier\" not in dir():\n",
    "    fail = True;\n",
    "    print(\"{} The create_classifier function has not been defined.\\033[0m\".format(issue))\n",
    "    print(\"{} Your code must include a create_classifier function as described in the coursework specification.\\033[0m\".format(error))\n",
    "    print(\"{} If you believe you have, \\'restart & run-all\\' to clear this error.\\033[0m\".format(error))\n",
    "else:\n",
    "    print('{} The create_classifier function has been defined.\\033[0m'.format(success))\n",
    "\n",
    "#######\n",
    "##\n",
    "## Classifier variable check.\n",
    "##\n",
    "## Test that checks the classifier variable exists. The marking system will use this variable to make predictions based on a set of random features you have not seen. Your score will be based on how well your classifier predicts the hidden labels.\n",
    "##\n",
    "#######\n",
    "\n",
    "if 'classifier' not in vars():\n",
    "    fail = True;\n",
    "    print(\"{} The classifer variable has not been defined.\\033[0m\".format(issue))\n",
    "    print(\"{} Your code must create a variable called \\'classifier\\' as described in the coursework specification.\\033[0m\".format(error))\n",
    "    print(\"{} This variable should contain the trained classifier you have created.\\033[0m\".format(error))\n",
    "else:\n",
    "    print('{} The classifer variable has been correctly defined.\\033[0m'.format(success))\n",
    "\n",
    "#######\n",
    "##\n",
    "## Accuracy Estimation check.\n",
    "##\n",
    "## Test that checks the accuracy estimation function exists and is a reasonable value. This is a requirement of the coursework specification and is used by the marking system when generating your final grade.\n",
    "##\n",
    "#######\n",
    "\n",
    "if \"my_accuracy_estimate\" not in dir():\n",
    "    fail = True;\n",
    "    print(\"{} The my_accuracy_estimate function has not been defined.\\033[0m\".format(issue))\n",
    "    print(\"{} Your code must include a my_accuracy_estimate function as described in the coursework specification.\\033[0m\".format(error))\n",
    "    print(\"{} If you believe you have, \\'restart & run-all\\' to clear this error.\\033[0m\".format(error))\n",
    "else:\n",
    "    if my_accuracy_estimate() == 0.5:\n",
    "        print(\"{} my_accuracy_estimate function warning.\\033[0m\".format(issue))\n",
    "        print(\"{} my_accuracy_estimate function returns a value of 0.5 - Your classifier is no better than random chance.\\033[0m\".format(error))\n",
    "        print(\"{} Are you sure this is correct.\\033[0m\".format(error))\n",
    "    else:\n",
    "        print('{} The my_accuracy_estimate function has been defined correctly.\\033[0m'.format(success))\n",
    "\n",
    "#######\n",
    "##\n",
    "## Test set check.\n",
    "##\n",
    "## Test that checks your classifier actually works. The calls made here are the same made by the automarker - albeit with different data. If your work fails this test it will score 0 in the automarker.\n",
    "##\n",
    "#######\n",
    "\n",
    "try:\n",
    "    testing_spam = np.loadtxt(open(\"data/testing_spam.csv\"), delimiter=\",\").astype(int)\n",
    "    test_data = testing_spam[:, 1:]\n",
    "    test_labels = testing_spam[:, 0]\n",
    "    \n",
    "    try:\n",
    "        predictions = classifier.predict(test_data)\n",
    "        accuracy = np.count_nonzero(predictions == test_labels)/test_labels.shape[0]\n",
    "        print('{0} Success running test set - Accuracy was {1:.2f}%.\\033[0m'.format(success, (accuracy*100)))\n",
    "    except Exception as e:\n",
    "        fail = True\n",
    "        print(\"{} Error running test set.\\033[0m\".format(issue))\n",
    "        print(\"{} Your code produced the following error. This error will result in a zero from the automarker, please fix.\\033[0m\".format(error))\n",
    "#         print(\"{} {}\\033[0m\".format(error, e))\n",
    "        print(e)\n",
    "except:\n",
    "    sys.stderr.write(\"Unable to run one test as the file \\'data/testing_spam.csv\\' could not be found.\")\n",
    "\n",
    "#######\n",
    "##\n",
    "## Final Summary\n",
    "##\n",
    "## Prints the final results of the submission tests.\n",
    "##\n",
    "#######\n",
    "\n",
    "if fail:\n",
    "    sys.stderr.write(\"Your submission is not ready! Please read and follow the instructions above.\")\n",
    "else:\n",
    "    print(\"\\033[1m\\n\\n\")\n",
    "    print(\"╔═══════════════════════════════════════════════════════════════╗\")\n",
    "    print(\"║                        Congratulations!                       ║\")\n",
    "    print(\"║                                                               ║\")\n",
    "    print(\"║            Your work meets all the required criteria          ║\")\n",
    "    print(\"║                   and is ready for submission.                ║\")\n",
    "    print(\"╚═══════════════════════════════════════════════════════════════╝\")\n",
    "    print(\"\\033[0m\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "136737e66133e8cd4881060775030b30",
     "grade": true,
     "grade_id": "cell-b64bc40ab6485b50",
     "locked": true,
     "points": 50,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell. Please do not modify or delete."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
